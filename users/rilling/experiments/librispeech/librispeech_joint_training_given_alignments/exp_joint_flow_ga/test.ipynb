{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/u/lukas.rilling/experiments/glow_tts_asr_v2/recipe/i6_experiments/users/rilling/experiments/librispeech/librispeech_joint_training_given_alignments/exp_joint_flow_ga/test.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/u/lukas.rilling/experiments/glow_tts_asr_v2/recipe/i6_experiments/users/rilling/experiments/librispeech/librispeech_joint_training_given_alignments/exp_joint_flow_ga/test.ipynb#W0sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m encodings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn((batch_size, \u001b[39mint\u001b[39m(phoneme_sequences\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]), encoding_size), requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# Example encodings\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/u/lukas.rilling/experiments/glow_tts_asr_v2/recipe/i6_experiments/users/rilling/experiments/librispeech/librispeech_joint_training_given_alignments/exp_joint_flow_ga/test.ipynb#W0sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Calculate the mean encoding for every phoneme\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/u/lukas.rilling/experiments/glow_tts_asr_v2/recipe/i6_experiments/users/rilling/experiments/librispeech/librispeech_joint_training_given_alignments/exp_joint_flow_ga/test.ipynb#W0sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m mean_encodings, distances \u001b[39m=\u001b[39m encoding_distance_loss(phoneme_sequences, encodings, sequence_lengths)\n\u001b[1;32m     <a href='vscode-notebook-cell:/u/lukas.rilling/experiments/glow_tts_asr_v2/recipe/i6_experiments/users/rilling/experiments/librispeech/librispeech_joint_training_given_alignments/exp_joint_flow_ga/test.ipynb#W0sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEncodings: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, encodings)\n\u001b[1;32m     <a href='vscode-notebook-cell:/u/lukas.rilling/experiments/glow_tts_asr_v2/recipe/i6_experiments/users/rilling/experiments/librispeech/librispeech_joint_training_given_alignments/exp_joint_flow_ga/test.ipynb#W0sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMean encodings shape:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, mean_encodings\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;32m/u/lukas.rilling/experiments/glow_tts_asr_v2/recipe/i6_experiments/users/rilling/experiments/librispeech/librispeech_joint_training_given_alignments/exp_joint_flow_ga/test.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/u/lukas.rilling/experiments/glow_tts_asr_v2/recipe/i6_experiments/users/rilling/experiments/librispeech/librispeech_joint_training_given_alignments/exp_joint_flow_ga/test.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m mask \u001b[39m=\u001b[39m sequence_mask(seq_lenghts, phoneme_sequences\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/u/lukas.rilling/experiments/glow_tts_asr_v2/recipe/i6_experiments/users/rilling/experiments/librispeech/librispeech_joint_training_given_alignments/exp_joint_flow_ga/test.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m phoneme_sequences_masked \u001b[39m=\u001b[39m ((phoneme_sequences \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m mask) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/u/lukas.rilling/experiments/glow_tts_asr_v2/recipe/i6_experiments/users/rilling/experiments/librispeech/librispeech_joint_training_given_alignments/exp_joint_flow_ga/test.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m encodings_masked \u001b[39m=\u001b[39m ((encodings \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m) \u001b[39m*\u001b[39;49m mask) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/u/lukas.rilling/experiments/glow_tts_asr_v2/recipe/i6_experiments/users/rilling/experiments/librispeech/librispeech_joint_training_given_alignments/exp_joint_flow_ga/test.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m encodings_flat \u001b[39m=\u001b[39m encodings_masked\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, encodings_masked\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/u/lukas.rilling/experiments/glow_tts_asr_v2/recipe/i6_experiments/users/rilling/experiments/librispeech/librispeech_joint_training_given_alignments/exp_joint_flow_ga/test.ipynb#W0sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Flatten the phoneme sequences tensor to match the encodings tensor\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from i6_experiments.users.rilling.experiments.librispeech.librispeech_joint_training_given_alignments.pytorch_networks.shared.commons import sequence_mask\n",
    "# Assuming you have your 2D tensor of phoneme sequences and 3D tensor of encodings\n",
    "# Let's call them phoneme_sequences and encodings respectively\n",
    "\n",
    "# Phoneme sequences tensor shape: [batch_size, sequence_length]\n",
    "# Encodings tensor shape: [batch_size, sequence_length, encoding_size]\n",
    "\n",
    "\n",
    "# Calculate the mean encoding for every phoneme across all batches\n",
    "def encoding_distance_loss(phoneme_sequences, encodings, seq_lenghts):\n",
    "    # Reshape the encodings tensor to [batch_size * sequence_length, encoding_size]\n",
    "    mask = sequence_mask(seq_lenghts, phoneme_sequences.shape[-1])\n",
    "    phoneme_sequences_masked = ((phoneme_sequences + 1) * mask) - 1\n",
    "    encodings_masked = ((encodings + 1) * mask) - 1\n",
    "\n",
    "    encodings_flat = encodings_masked.view(-1, encodings_masked.size(-1))\n",
    "\n",
    "    # Flatten the phoneme sequences tensor to match the encodings tensor\n",
    "    phoneme_sequences_flat = phoneme_sequences_masked.view(-1)\n",
    "\n",
    "    phonemes, inverse, counts = torch.unique(phoneme_sequences_flat, return_counts=True, return_inverse=True)\n",
    "\n",
    "    # Initialize a tensor to store the sum of encodings for each phoneme\n",
    "    sum_encodings = torch.zeros(phonemes.shape[0], encodings.size(-1))\n",
    "\n",
    "    # Use index_add to collect the sum of all phoneme encodings in the current batch\n",
    "    sum_encodings = sum_encodings.index_add(0, inverse, encodings_flat)\n",
    "\n",
    "    mean_encodings = (sum_encodings / counts.unsqueeze(1)).unsqueeze(0)\n",
    "\n",
    "    loss = -1 * torch.cdist(mean_encodings, mean_encodings).sum()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Example usage\n",
    "batch_size = 2\n",
    "sequence_lengths = torch.Tensor([4, 5])\n",
    "encoding_size = 5\n",
    "\n",
    "# Create example tensors\n",
    "phoneme_sequences = torch.tensor([[0, 1, 5, 2, 0, 0], [3, 0, 2, 5, 6, 0]], dtype=torch.float32, requires_grad=True)  # Example phoneme sequences\n",
    "print(phoneme_sequences.shape[-1])\n",
    "encodings = torch.randn((batch_size, int(phoneme_sequences.shape[-1]), encoding_size), requires_grad=True)  # Example encodings\n",
    "\n",
    "# Calculate the mean encoding for every phoneme\n",
    "mean_encodings, distances = encoding_distance_loss(phoneme_sequences, encodings, sequence_lengths)\n",
    "\n",
    "print(\"Encodings: \\n\", encodings)\n",
    "print(\"Mean encodings shape:\\n\", mean_encodings.shape)\n",
    "print(\"Mean encodings:\\n\", mean_encodings)\n",
    "print(\"Encoding for 3: \\n\", encodings[1, 0])\n",
    "print(\"Mean encoding for 3:\\n\", mean_encodings[3])\n",
    "print(\"Distance: \", distances)\n",
    "distances.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9826,  1.0340,  0.7152,  0.6475, -0.7005])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(encodings[0, 0] + encodings[1, 1])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  0., -1., -1., -1.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.Tensor([0, 1, 2, 0, 6, 8, 8])\n",
    "mask = torch.Tensor([1, 1, 1, 1, 0, 0, 0])\n",
    "test_masked = ((test + 1) * mask) - 1\n",
    "test_masked"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
